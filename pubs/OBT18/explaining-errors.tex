% -*- compile-command: "pdflatex explaining-errors.tex" -*-

%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
\documentclass[sigplan, screen]{acmart}\settopmatter{printccs=false,printacmref=false}


\newif\ifcomments\commentstrue
\ifcomments
\newcommand{\rae}[1]{\textcolor{magenta}{RAE: #1}}
\else
\newcommand{\rae}[1]{}
\fi

% https://tex.stackexchange.com/questions/346292/remove-conference-information-from-acm-2017-sigconf-template
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers

\setcopyright{none}

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
\citestyle{acmauthoryear}   %% For author/year citations
%\citestyle{acmnumeric}     %% For numeric citations
%\setcitestyle{nosort}      %% With 'acmnumeric', to disable automatic
                            %% sorting of references within a single citation;
                            %% e.g., \cite{Smith99,Carpenter05,Baker12}
                            %% rendered as [14,5,2] rather than [2,5,14].
%\setcitesyle{nocompress}   %% With 'acmnumeric', to disable automatic
                            %% compression of sequential references within a
                            %% single citation;
                            %% e.g., \cite{Baker12,Baker14,Baker16}
                            %% rendered as [2,3,4] rather than [2-4].

%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption

\usepackage{mathpartir}
\usepackage{acode}

\usepackage{wasysym}  %% for \frownie

\usepackage{xcolor}

\begin{document}

\title{Explaining Type Errors}

%% Author with single affiliation.
\author{Brent A. Yorgey}
\affiliation{
  \institution{Hendrix College}
}
\email{yorgey@hendrix.edu}

\author{Richard A. Eisenberg}
\affiliation{
  \institution{Bryn Mawr College}
}
\email{rae@cs.brynmawr.edu}

\author{Harley D. Eades III}
\affiliation{
  \institution{Augusta University}
}
\email{heades@augusta.edu}

% %% Keywords
% \keywords{keyword1, keyword2, keyword3}


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle

Every beginning student of programming---that is, every student with
the ill fortune of having a language with a static type system foisted
upon them by a well-intentioned yet sadistic instructor---is
well-acquainted with the Dreaded Type Error Message:
{\small
\begin{verbatim}
Couldn't match expected type (t, b0) with actual type Int
In the first argument of fst, namely p
  In the expression: fst p
  In the first argument of \ f -> f (3 :: Int), namely
    (\ p -> fst p)
\end{verbatim}
}
Why do type error messages have to be so terrifying?  Can't we do a
better job explaining type errors to programmers?

We propose two interrelated theses:
\begin{enumerate}
\item We ought to move away from static error ``messages'' and towards
  \emph{interactive error explanations}.
\item We ought to consider the problem of generating error
  explanations in a more \emph{systematic, disciplined, and formal way.}
  Explaining errors to users shouldn't just be relegated to the status
  of an ``engineering issue'', but ought to have all the tools of
  programming language theory and practice applied to it.
\end{enumerate}

\section{The curse of information}

Consider a standard version of the simply typed lambda calculus shown
in Figure~\ref{fig:STLC}, with
some arbitrary set of base types $B$ and typing annotations on
lambda-bound variables.

\newcommand{\lam}[3]{\lambda {#1}\!:\!{#2}.\; {#3}}
\newcommand{\app}[2]{{#1}\; {#2}}

\newcommand{\ty}[3]{{#1} \vdash {#2} : {#3}}
\newcommand{\nty}[3]{{#1} \nvdash {#2} : {#3}}

\begin{figure}[h!]
\begin{align*}
  t &::= x \mid \lam x \tau t \mid \app{t_1}{t_2} \\
  \tau &::= B \mid \tau_1 \to \tau_2 \\
  \Gamma &::= \cdot \mid \Gamma,x:\tau
\end{align*}

\begin{mathpar}
  \inferrule{x : \tau \in \Gamma}{\ty \Gamma x \tau} \and
  \inferrule{\ty {\Gamma, x:\tau_1} t {\tau_2}}{\ty \Gamma {\lam x
      {\tau_1} t}{\tau_1 \to \tau_2}} \and
  \inferrule{\ty \Gamma {t_1} {\tau_1 \to \tau_2} \\ \ty \Gamma {t_2} {\tau_1}}
            {\ty \Gamma {\app{t_1}{t_2}} {\tau_2}}
\end{mathpar}
\caption{The simply typed lambda calculus} \label{fig:STLC}
\end{figure}

A typical implementation of a type checker for this language recurs
through the structure of a term, adding bindings to the context as it
recurs through lambdas, and checking that the types match up
appropriately at each application.  If the types don't match, some
sort of error message is generated: \medskip

\begin{acode}
  \> !if ($\tau_1 \neq \tau_1'$) \\
  \> \tb \> !then \> !throwError ``Mismatch: expected \{$\tau_1$\}, actual \{$\tau_1'$\}'' \\
  \> !else \> ...
\end{acode} \medskip

Of course, this error message lacks any sort of context.  One problem
is that it may be difficult for the programmer to even figure out
which part of their program the error corresponds to, especially if
the mismatch happened deep inside a large term. This problem is not
too hard to solve, by retaining information about the source code
locations of terms, and possibly by making use of appropriate editor
support for highlighting the locations of reported error messages; of
course, most real-world language implementations actually do this.

However, a deeper problem is that even if the programmer knows
\emph{where} in their program the error message came from, they may
not understand \emph{why} it is an error: for example, where
did the two types in question come from, and why does the type checker
think they ought to be equal?

A natural reaction to this problem is to add more information to the
error message: for example, highlighting a larger portion of the term
containing the problematic subterm, including the types of variables
mentioned in the term, or even giving suggestions about potential
fixes.  However, this is likely to be unhelpful in the long run:

\begin{itemize}
\item How do we decide which information to include in a given
  message?  If we include all possible information (whatever that even
  means!), the message will be impossibly large. On the other hand, if
  we are more selective, we may end up omitting information which
  would have been helpful---not to mention that the helpfulness of any
  particular information varies depending on the individual programmer
  and their background.
\item Even if we somehow figure out which information would be most
  helpful to include, paradoxically, it may not be helpful to include
  it!  Beginners, in particular, may be \emph{less} likely to read
  large error messages---even when those error messages contain
  information that would genuinely help them---because the messages seem
  overwhelming or intimidating. Experts also may prefer less
  information, because in many cases they do not need it, and they
  would rather be able to see more error messages on the screen at
  once.
\end{itemize}

It seems we can't win: programmers will be confused if there is not enough
information about what went wrong; but if we try to present more
information, it is likely to be unhelpful, overwhelming, or both.

\section{Interactivity to the rescue}

The problem of how much information to include in error messages is
actually a red herring.  The real problem is that for reasons of
history and technical convenience, we are stuck thinking in a
framework that is terminal-based and oriented towards batch
processing.  Even the term ``error \emph{message}'' itself seems to
presuppose this mode of interaction.

Suppose, instead, that the programmer is allowed to \emph{explore errors
  interactively}: they are initially presented with a concise
description of the error, and then they can incrementally explore
additional information---for example, by expanding nodes in a tree
corresponding to questions they might want to ask.  This solves both
problems outlined above:
\begin{itemize}
\item We do not have to decide what information to include up front;
  we can think of the error explanation as a lazy tree containing all
  the information we could ever conceivably generate about the error
  and its causes.  The programmer then gets to interactively select
  exactly the information they want to see.
\item The programmer is less likely to be overwhelmed, since the
  initial message they are shown can be kept short and to the point.
  Even if they end up looking at the same information that a static,
  batch error message might have included, processing and assimilating
  the information will still be psychologically easier when they
  are able to explore the information incrementally.
\end{itemize}

% \newenvironment{discomsg}{\begin{quote}\sffamily}{\end{quote}}
% \newcommand{\discoq}{\item[$\blacktriangleright$]}
% \newcommand{\discoqa}{\item[$\blacktriangledown$]}

% For example, suppose \verb|f| is a function expecting a pair of
% natural numbers, but a programmer writes \verb|f b|, where \verb|b|
% has been inferred to have type \verb|Nat|.  They might be shown
% something like this:
% \begin{discomsg}
%   Type error: `\verb|b|' is a natural number, but it is used in a
%   place where there should be a pair of natural numbers.
%   \begin{enumerate}
%     \discoq Why is `\verb|b|' a natural number?
%     \discoq Why should there be a pair of natural numbers here instead?
%   \end{enumerate}
% \end{discomsg}
% The programmer can then expand either question to see an answer along
% with further questions for exploring more deeply.  For example,
% {\sffamily Why is `\verb|b|' a natural number?} might expand into an
% explanation of where \verb|b| is bound, along with further questions
% exploring how the binding site of \verb|b| affects its inferred type.

\section{How to explain things}

So, how do we go about generating such error explanations?  It is here
that we think the tools of programming language theory can be
fruitfully applied. On the face of it, type errors do not seem like a
very ``off the beaten track'' topic at all; but despite their close
connection to programming languages and type systems, we are not
actually aware of any work taking a principled, PL-style approach to
the problem of explaining type errors to programmers.

When a program has a valid type, how do we explain it?  The answer to
this is well-known in the PL community: a \emph{typing derivation} is
a (constructive) proof that a given term has a given type, and a
constructive proof is simply a detailed, logically
rigorous explanation.  If we want a type inference algorithm to
explain itself, we can have it return an entire typing derivation
rather than just a type.

So, when a program \emph{doesn't} type check, how should we explain
it?  Given the discussion in the previous paragraph, the answer ought
to be clear: with an \emph{untyping derivation}, that is, a
constructive proof of \emph{un}typability!  We can extract information
from this to show to the programmer, or we can simply let them
interactively explore it.  That is, to be clear, we propose that an
untyping derivation itself should form the core of the tree structure
making up an error explanation (though actually it should probably be
more like a zipper onto the untyping derivation, since the explanation
should start ``focused in'' on the specific location of an error,
rather than on the top-level program).

Let's look at a simple example.  Figure~\ref{fig:untyping-STLC}
shows just two of the rules for untyping derivations for the simply typed
lambda calculus; we pronounce $\nty \Gamma t \tau$ as ``$t$ does
not have type $\tau$ in context $\Gamma$''.
\begin{figure}
\begin{mathpar}
  % % -- ƛ-funty holds if τ is not a function type at all, or if it is a
  % % -- function type whose input type is not τ₁.
  % % ƛ-funty  : ∀ {n} {Γ : Ctx n} {t} {τ₁ τ}
  % %            → (∀ {τ₂} → τ ≁ τ₁ ⇒ τ₂)
  % %            → Γ ⊬ ƛ τ₁ t ∶ τ
  % \inferrule*[right=AbsTy\frownie{}]
  %   {\forall \tau_2.\; \tau \neq (\tau_1 \to \tau_2)}
  %   {\nty \Gamma {\lam x {\tau_1} t} \tau}
  %   \and

  % % -- Otherwise, τ is of the form (τ₁ ⇒ τ₂) but the body t does not
  % % -- have type τ₂.  Note this could be either because t is not typable
  % % -- at all, or because it has some type other than τ₂.
  % % ƛ        : ∀ {n} {Γ : Ctx n} {t} {τ₁ τ₂}
  % %            → (τ₁ ∷ Γ) ⊬ t ∶ τ₂
  % %            → Γ ⊬ ƛ τ₁ t ∶ (τ₁ ⇒ τ₂)
  % \inferrule*[right=AbsBody\frownie{}]
  %   {\nty{\Gamma, x:\tau_1} {t} {\tau_2}}
  %   {\nty \Gamma {\lam x {\tau_1} t} {\tau_1 \to \tau_2}}
  %   \and

  % ·-fun    : ∀ {n} {Γ : Ctx n} {t₁ t₂} {τ₂}
  %            → (∀ {τ₁} → Γ ⊬ t₁ ∶ τ₁ ⇒ τ₂)
  %            → Γ ⊬ t₁ · t₂ ∶ τ₂
  \inferrule*[right=LhsTy\frownie{}]
    {\forall \tau_1.\; \nty \Gamma {t_1} {\tau_1 \to \tau_2}}
    {\nty \Gamma {\app {t_1} {t_2}} {\tau_2}}
    \and

  % ·-arg    : ∀ {n} {Γ : Ctx n} {t₁ t₂} {τ₁ τ₂}
  %            → Γ ⊢ t₁ ∶ τ₁ ⇒ τ₂
  %            → Γ ⊬ t₂ ∶ τ₁
  %            → Γ ⊬ t₁ · t₂ ∶ τ₂
  \inferrule*[right=RhsTy\frownie{}]
    {\ty \Gamma {t_1} {\tau_1 \to \tau_2} \\
      \nty \Gamma {t_2} {\tau_1}}
    {\nty \Gamma {\app {t_1} {t_2}} {\tau_2}}
  \end{mathpar}
  \caption{Some untyping rules for the STLC} \label{fig:untyping-STLC}
\end{figure}
% Rules \textsc{AbsTy\frownie{}} and \textsc{AbsBody\frownie{}}
% enumerate the ways that a lambda term can fail to have a given type:
% the first says that if $\tau$ is not a function type, then a lambda
% term does not have type $\tau$; the second says that if the body of
% the lambda does not have type $\tau_1$ in a suitably extended context,
% then the whole lambda does not have type $\tau_1 \to \tau_2$.
Rules \textsc{LhsTy\frownie{}} and \textsc{RhsTy\frownie{}} enumerate
the ways that an application can fail to have a given type:
$\app{t_1}{t_2}$ does not have type $\tau_2$ if either $t_1$ does not
have an appropriate function type, or if $t_1$ does have an
appropriate type but $t_2$ does not have the right type to be its
argument. \rae{I'm a bit concerned by the fact that the lhs rule isn't
actually inductive, having a $\forall$ in it. Note that it would take
an infinite number of premises to prove LhsTy. As long as we're not
doing metatheory proofs over untyping derivations, this might be OK.
But at the least, if you agree with this concern, we should mention
in the text that the $\forall$ is a soft spot in the theory.}

How do we know when we have the definition of untyping derivations
correct for a given type system?  We can justify a particular
definition of untyping by proving some metatheorems relating it to
typing.  For example, we certainly want a metatheorem of the form
\[ \forall t \tau.\; (\nty \Gamma t {\tau}) \implies \neg(\ty \Gamma t
  {\tau}). \] For some type systems we can prove the converse as well.
If we want an untyping derivation to somehow encompass all possible
errors in a term, rather than just picking one (corresponding to the
difference between a type checking algorithm that stops as soon as an
error is encountered and one which tries to continue and collect
additional errors), we could try to prove a uniqueness theorem---for a
given triple of $(\Gamma, t, \tau)$ there is at most one untyping
derivation.

There are many remaining questions:
\begin{itemize}
\item There is still a lot of latitude in designing the rules for
  untyping derivations.  For example, one could imagine adding another
  rule for the STLC:
  \begin{mathpar}
    \inferrule{\ty \Gamma {t_1} {\tau} \\ \textsc{NotArrow}(\tau)}{\nty
      \Gamma {\app{t_1}{t_2}} {\tau_2}}
  \end{mathpar}
  This rule says that an application does not have a type if its LHS
  \emph{does} have a type, but not an arrow type.  This rule is
  actually subsumed by rule \textsc{LhsTy\frownie{}}, but might be
  more comprehensible to programmers in cases when it applies.  Is
  there a systematic way to approach the task of choosing rules for
  building untyping derivations?
\item Is there some way to \emph{mechanically} derive untyping
  derivation rules from the rules of a type system?  De Morgan-type
  laws may play a starring role, but there are likely to be
  subtleties, given considerations such as the previous bullet point.
\item Although building appropriate untyping rules seems straightforward
  for natural-deduction-style systems, it becomes much trickier when
  trying to explain unification failures; the precise algorithms used
  to do unification seem to matter quite a bit.  What is the right way
  to explain unification failures to programmers?
\end{itemize}

\section{Proposal}

In our talk, we will (a) motivate interactive error explanations and
untyping derivations, (b) present several variant systems of untyping
rules for the STLC, with appropriate metatheorems all formalized in
Agda, (c) share some preliminary thoughts on how to explain
unification errors.

% How do we know what the right definition of untyping derivations is?  Simple: we know we've "gotten it right" when we can prove a metatheorem showing that there is an untyping derivation if and only if there is not a typing derivation (or maybe we only want one direction if the type system is sufficiently undecidable??).  At this point I can pull out a simple example I've coded up in Agda, with typing and untyping derivations for the STLC, plus a formal metatheorem relating them.  (In practice, I didn't get the definition of untyping proofs right the first time: I had to adjust it when my proof didn't go though!  A triumph for machine-checked formalism.)

% Something about history and natural deduction blah blah, the natural-deduction style proofs we use for typing derivations just so happen to correspond very nicely to the way humans want to reason/think about things, and all the thinking can be done "locally".  Unfortunately this seems to go out the window when we bring in things like unification, because it is inherently nonlocal.

% I have a few thoughts about ways to design systems with unification to make them more explainable.  One of the main thoughts is that using a union-find data structure is not just for speed---it's also much more explainable!  Humans don't want to think in terms of transitive chains of equalities or in terms of (shudder) substitutions.  If the name of a thing keeps changing during the course of an explanation, a human is going to get very confused.  Instead we can keep the names of things the same and just accumulate sets of names which are all equal.  I also have some thoughts on using "extensional" names instead of/in addition to fresh ones (e.g. using a name like "the argument type of f" instead of "a0" or whatever).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% %% Acknowledgments
% \begin{acks}                            %% acks environment is optional
%                                         %% contents suppressed with 'anonymous'
%   %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
%   %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
%   %% acknowledge financial support and will be used by metadata
%   %% extraction tools.
%   This material is based upon work supported by the
%   \grantsponsor{GS100000001}{National Science
%     Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
%   No.~\grantnum{GS100000001}{nnnnnnn} and Grant
%   No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
%   conclusions or recommendations expressed in this material are those
%   of the author and do not necessarily reflect the views of the
%   National Science Foundation.
% \end{acks}


\bibliography{explaining-errors}


\end{document}
